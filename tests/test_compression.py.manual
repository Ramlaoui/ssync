#!/usr/bin/env python3
"""Test script for compressed output functionality."""

import asyncio
import base64
import gzip
import sys
from pathlib import Path

import pytest

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from src.ssync.cache import get_cache
from src.ssync.utils.logging import setup_logger

logger = setup_logger(__name__)


def test_compression():
    """Test compression on remote host."""
    print("\n=== Testing Compression Functionality ===\n")

    # Test data
    test_cases = [
        {
            "name": "Small file (< 1KB)",
            "content": "Hello, World!\n" * 10,
            "should_compress": False,
        },
        {
            "name": "Medium file (> 1KB)",
            "content": "x" * 2000 + "\n",
            "should_compress": True,
        },
        {
            "name": "Large file with repetition",
            "content": ("This is a test line that repeats.\n" * 1000),
            "should_compress": True,
        },
    ]

    for case in test_cases:
        print(f"\nTest: {case['name']}")
        print(f"Original size: {len(case['content'])} bytes")

        # Simulate compression
        compressed = gzip.compress(case["content"].encode("utf-8"))
        ratio = (1 - len(compressed) / len(case["content"])) * 100

        print(f"Compressed size: {len(compressed)} bytes")
        print(f"Compression ratio: {ratio:.1f}%")
        print(f"Should compress: {case['should_compress']}")

        # Test base64 encoding
        encoded = base64.b64encode(compressed).decode("ascii")
        print(f"Base64 encoded size: {len(encoded)} bytes")

        # Test decompression
        decoded = base64.b64decode(encoded)
        decompressed = gzip.decompress(decoded)
        assert decompressed.decode("utf-8") == case["content"], "Decompression failed!"
        print("✓ Compression/decompression successful")


def test_cache_storage():
    """Test storing compressed data in cache."""
    print("\n=== Testing Cache Storage ===\n")

    cache = get_cache()

    # Test data
    job_id = "test_job_123"
    hostname = "test_host"
    test_output = "This is test output\n" * 100

    # Compress
    compressed = gzip.compress(test_output.encode("utf-8"))
    encoded = base64.b64encode(compressed).decode("ascii")

    # Create test data dict
    stdout_data = {
        "compressed": True,
        "data": encoded,
        "original_size": len(test_output),
        "compression": "gzip",
    }

    print(f"Storing compressed output for job {job_id}")
    print(f"Original size: {len(test_output)} bytes")
    print(f"Compressed size: {len(compressed)} bytes")

    # First create a job entry
    from datetime import datetime

    from src.ssync.models.job import JobInfo, JobState

    job_info = JobInfo(
        job_id=job_id,
        name="test_job",
        user="test_user",
        state=JobState.COMPLETED,
        hostname=hostname,
        submit_time=datetime.now(),
        start_time=datetime.now(),
        runtime="00:01:00",
    )

    # Cache the job first
    from src.ssync.cache import CachedJobData

    cached_data = CachedJobData(
        job_id=job_id,
        hostname=hostname,
        job_info=job_info,
        stdout_compressed=base64.b64decode(encoded),
        stdout_size=len(test_output),
        stdout_compression="gzip",
    )

    # Store directly
    cache.cache_job(cached_data)

    # Retrieve from cache
    cached_job = cache.get_cached_job(job_id, hostname)

    if cached_job:
        print("✓ Retrieved from cache")
        print(f"Cached stdout size: {cached_job.stdout_size} bytes")
        print(f"Cached compression: {cached_job.stdout_compression}")

        # Verify decompression
        if cached_job.stdout_compressed:
            decompressed = gzip.decompress(cached_job.stdout_compressed)
            assert decompressed.decode("utf-8") == test_output, (
                "Cache retrieval failed!"
            )
            print("✓ Cache storage/retrieval successful")
    else:
        print("✗ Failed to retrieve from cache")


@pytest.mark.asyncio
async def test_streaming():
    """Test streaming functionality."""
    print("\n=== Testing Streaming ===\n")

    import io

    # Create test compressed data
    test_data = ("Line {}\n".format(i) for i in range(1000))
    test_content = "".join(test_data)
    compressed = gzip.compress(test_content.encode("utf-8"))

    print(f"Test content size: {len(test_content)} bytes")
    print(f"Compressed size: {len(compressed)} bytes")

    # Simulate streaming decompression
    chunk_size = 1024
    chunks_read = 0
    total_bytes = 0

    dobj = gzip.GzipFile(fileobj=io.BytesIO(compressed))

    print(f"\nStreaming with {chunk_size} byte chunks:")
    while True:
        chunk = dobj.read(chunk_size)
        if not chunk:
            break
        chunks_read += 1
        total_bytes += len(chunk)
        if chunks_read <= 3:
            print(f"  Chunk {chunks_read}: {len(chunk)} bytes")

    print(f"\nTotal chunks: {chunks_read}")
    print(f"Total bytes streamed: {total_bytes}")
    assert total_bytes == len(test_content), "Streaming failed!"
    print("✓ Streaming successful")


def main():
    """Run all tests."""
    print("=" * 60)
    print("Testing Compression and Streaming Functionality")
    print("=" * 60)

    try:
        # Test compression
        test_compression()

        # Test cache storage
        test_cache_storage()

        # Test streaming
        asyncio.run(test_streaming())

        print("\n" + "=" * 60)
        print("✓ All tests passed successfully!")
        print("=" * 60)

    except Exception as e:
        print(f"\n✗ Test failed: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
